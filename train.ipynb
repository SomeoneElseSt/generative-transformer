{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4ba3e3",
   "metadata": {},
   "source": [
    "If I'm going to train a model it should be interesting. \n",
    "\n",
    "\n",
    "It'll use this [dataset] (https://www.kaggle.com/datasets/rtatman/state-of-the-union-corpus-1989-2017?select=Eisenhower_1960.txt) of State of the Union addresses from 1780 to 2018. \n",
    "\n",
    "Let's make a politician :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574c67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to run preprocessing.py first\n",
    "\n",
    "with open('dataset.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc158ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in dataset: 10602521\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of characters in dataset: {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675d6be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gentlemen of the Senate and Gentlemen of the House of Representatives:\n",
      "\n",
      "I was for some time apprehensive that it would be necessary, on account of\n",
      "the contagious sickness which afflicted the city of P\n"
     ]
    }
   ],
   "source": [
    "# See first 200 characters\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c75262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique characters in this dataset: \t\n",
      " !\"$%&'()*+,-./0123456789:;=?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]_`abcdefghijklmnopqrstuvwxyz\n",
      "\n",
      "Lenght of unique characters: 89\n"
     ]
    }
   ],
   "source": [
    "# Prints all unique chars in the dataset in order \n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"All unique characters in this dataset: {''.join(chars)}\\n\")\n",
    "print(f\"Lenght of unique characters: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e6706a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string [54, 77, 80, 66, 81]\n",
      "\n",
      "Decoded string: Words\n"
     ]
    }
   ],
   "source": [
    "# Map characters to integers and viceversa\n",
    "# Only as many mapping as available chars \n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# Encoder: str -> int. Decoder: int -> str\n",
    "# stoi: mapping to encode str -> int. itos: mapping to reverse map int -> str.\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "encoded = encode(\"Words\")\n",
    "print(f\"Encoded string {encoded}\\n\")\n",
    "\n",
    "decoded = decode(encoded)\n",
    "print(f\"Decoded string: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c6a4a1",
   "metadata": {},
   "source": [
    "Note from video: there are many other tokenizers, eg. sentencepiece (google), tiktoken (openai). They vary in the extent to which they break a word and how they map it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fc1ed6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10602521]) torch.int64\n",
      "tensor([38, 67, 76, 82, 74, 67, 75, 67, 76,  2, 77, 68,  2, 82, 70, 67,  2, 50,\n",
      "        67, 76, 63, 82, 67,  2, 63, 76, 66,  2, 38, 67, 76, 82, 74, 67, 75, 67,\n",
      "        76,  2, 77, 68,  2, 82, 70, 67,  2, 39, 77, 83, 81, 67,  2, 77, 68,  2,\n",
      "        49, 67, 78, 80, 67, 81, 67, 76, 82, 63, 82, 71, 84, 67, 81, 27,  1,  1,\n",
      "        40,  2, 85, 63, 81,  2, 68, 77, 80,  2, 81, 77, 75, 67,  2, 82, 71, 75,\n",
      "        67,  2, 63, 78, 78, 80, 67, 70, 67, 76, 81, 71, 84, 67,  2, 82, 70, 63,\n",
      "        82,  2, 71, 82,  2, 85, 77, 83, 74, 66,  2, 64, 67,  2, 76, 67, 65, 67,\n",
      "        81, 81, 63, 80, 87, 13,  2, 77, 76,  2, 63, 65, 65, 77, 83, 76, 82,  2,\n",
      "        77, 68,  1, 82, 70, 67,  2, 65, 77, 76, 82, 63, 69, 71, 77, 83, 81,  2,\n",
      "        81, 71, 65, 73, 76, 67, 81, 81,  2, 85, 70, 71, 65, 70,  2, 63, 68, 68,\n",
      "        74, 71, 65, 82, 67, 66,  2, 82, 70, 67,  2, 65, 71, 82, 87,  2, 77, 68,\n",
      "         2, 47])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Torch tensor creates an array with the encoded dataset.\n",
    "# A tensor is basically an array that supports GPU computing\n",
    "# It's faster for ML/AI and torch comes with configs for this specifically\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:200])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef266ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be trained with 9542268 characters, 1060253 will be set aside for validation\n"
     ]
    }
   ],
   "source": [
    "n = int(0.9 * len(data))\n",
    "\n",
    "# Keep only the 90th of the data for the training data\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "print(f\"The model will be trained with {n} characters, {len(data) - n} will be set aside for validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf5909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([38, 67, 76, 82, 74, 67, 75, 67])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Params\n",
    "\n",
    "# How many chars to sample from data for each training run\n",
    "# block_size matters because it will be the upper bound of how much the transformer will infer off of \n",
    "# If it gets inputted anything longer than this it might have a hard time fitting what is **expected** to come afterwards, because it did not see that during training\n",
    "block_size = 8\n",
    "train_data[:block_size+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00885056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([38, 67, 76, 82, 74, 67, 75, 67])\n",
      "tensor([67, 76, 82, 74, 67, 75, 67, 76])\n",
      "Input tensor([38]) | Target 67\n",
      "Input tensor([38, 67]) | Target 76\n",
      "Input tensor([38, 67, 76]) | Target 82\n",
      "Input tensor([38, 67, 76, 82]) | Target 74\n",
      "Input tensor([38, 67, 76, 82, 74]) | Target 67\n",
      "Input tensor([38, 67, 76, 82, 74, 67]) | Target 75\n",
      "Input tensor([38, 67, 76, 82, 74, 67, 75]) | Target 67\n",
      "Input tensor([38, 67, 76, 82, 74, 67, 75, 67]) | Target 76\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "\n",
    "# X is the normal array of block_size\n",
    "# Y is X shifted one space left (by passing start = 1 it skips the 0-th index)\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# This creates a relationship where the i-th x element precedes the i-th y element \n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] \n",
    "    target = y[t]  \n",
    "    print(f\"Input {context} | Target {target}\")\n",
    "\n",
    "# This loops print x up to the current t bound \n",
    "# And the char at the t - 1 position that goes after in the sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f92d89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[70, 80, 67, 67,  2, 69, 83, 76],\n",
      "        [66,  2, 71, 76, 15,  2, 51, 70],\n",
      "        [82, 70, 67, 80, 74, 63, 76, 66],\n",
      "        [ 2, 82, 70, 67,  2, 64, 71, 69]])\n",
      "Targets\n",
      "torch.Size([4, 8])\n",
      "tensor([[80, 67, 67,  2, 69, 83, 76, 64],\n",
      "        [ 2, 71, 76, 15,  2, 51, 70, 67],\n",
      "        [70, 67, 80, 74, 63, 76, 66, 81],\n",
      "        [82, 70, 67,  2, 64, 71, 69, 69]])\n",
      "Input tensor([70]) | Target 80\n",
      "Input tensor([70, 80]) | Target 67\n",
      "Input tensor([70, 80, 67]) | Target 67\n",
      "Input tensor([70, 80, 67, 67]) | Target 2\n",
      "Input tensor([70, 80, 67, 67,  2]) | Target 69\n",
      "Input tensor([70, 80, 67, 67,  2, 69]) | Target 83\n",
      "Input tensor([70, 80, 67, 67,  2, 69, 83]) | Target 76\n",
      "Input tensor([70, 80, 67, 67,  2, 69, 83, 76]) | Target 64\n",
      "Input tensor([66]) | Target 2\n",
      "Input tensor([66,  2]) | Target 71\n",
      "Input tensor([66,  2, 71]) | Target 76\n",
      "Input tensor([66,  2, 71, 76]) | Target 15\n",
      "Input tensor([66,  2, 71, 76, 15]) | Target 2\n",
      "Input tensor([66,  2, 71, 76, 15,  2]) | Target 51\n",
      "Input tensor([66,  2, 71, 76, 15,  2, 51]) | Target 70\n",
      "Input tensor([66,  2, 71, 76, 15,  2, 51, 70]) | Target 67\n",
      "Input tensor([82]) | Target 70\n",
      "Input tensor([82, 70]) | Target 67\n",
      "Input tensor([82, 70, 67]) | Target 80\n",
      "Input tensor([82, 70, 67, 80]) | Target 74\n",
      "Input tensor([82, 70, 67, 80, 74]) | Target 63\n",
      "Input tensor([82, 70, 67, 80, 74, 63]) | Target 76\n",
      "Input tensor([82, 70, 67, 80, 74, 63, 76]) | Target 66\n",
      "Input tensor([82, 70, 67, 80, 74, 63, 76, 66]) | Target 81\n",
      "Input tensor([2]) | Target 82\n",
      "Input tensor([ 2, 82]) | Target 70\n",
      "Input tensor([ 2, 82, 70]) | Target 67\n",
      "Input tensor([ 2, 82, 70, 67]) | Target 2\n",
      "Input tensor([ 2, 82, 70, 67,  2]) | Target 64\n",
      "Input tensor([ 2, 82, 70, 67,  2, 64]) | Target 71\n",
      "Input tensor([ 2, 82, 70, 67,  2, 64, 71]) | Target 69\n",
      "Input tensor([ 2, 82, 70, 67,  2, 64, 71, 69]) | Target 69\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1234)\n",
    "batch_size = 4 # How many sequences to process in each training run \n",
    "block_size = 8 # The maximum context lenght for each prediction \n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    # Creates random offsets to sample blocks from \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    # Y is the tensor that is offset by one for prediction\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    # torch.stack stacks by rows all the pairs\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "print(\"Inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b,:t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"Input {context} | Target {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8d531",
   "metadata": {},
   "source": [
    "Some notes so far:\n",
    "\n",
    "The core of the code above is separating the original encoded train set into two splits: one that is the same as the original (X) and one that is offset by 1 to the left (Y).\n",
    "\n",
    "The offset by 1 tensor will be used as the 'target'. The transformer will see pairs of varying block sizes with X as the base and Y as the target.\n",
    "\n",
    "This makes sense because the i-th offset tensor has the token that goes after the i-th X token **in the original array**. \n",
    "\n",
    "We will adjust the weights to lower the loss function depending on the accuracy of the model's predictions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac11abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 89])\n",
      "tensor(5.0302, grad_fn=<NllLossBackward0>)\n",
      "\tMxRl&__ESAn\n",
      ")vay1V&fQ\n",
      "lWJ]Ep/0r=A-@Wcx7:U!GzYuatrUUYj](IUG`-u1F2@8Ordz+];dugM[o\\=Lp%:agj7CP6r5T\t?hlR\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        logits = self.token_embedding_table(idx)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss \n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357ca6b",
   "metadata": {},
   "source": [
    "Notes on Bigram:\n",
    "\n",
    "Not sure how this works right now. Should watch Karpathy's videos on that.\n",
    "\n",
    "What I do know is that the cross entropy is basically the difference (from The Demon in the Machine: the randomness) between the input and the target. \n",
    "\n",
    "# Todo\n",
    "- softmax = ?\n",
    "- cross-entropy = ?\n",
    "- B, T, C and re-shaping = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27f7a528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam pytorch optimizer. Used for gradient descent\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eccf7b3",
   "metadata": {},
   "source": [
    "The code cell below uses Adam as an optimizer to lower the loss function of the X-th and Y-th tokens without using attention (ie. this is only done at the next-token level).\n",
    "\n",
    "# Todo:\n",
    "- zero_grad = ?\n",
    "- loss.backward = ?\n",
    "- how does .step() do what it does = ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec538a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.47403621673584\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # Fetch some data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # Gets the gradients \n",
    "    loss.backward()\n",
    "    # This descends using adam at each step\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0af67236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEGofueses ofe-y f\n",
      "mbutereconccongrs 1 thesthefomywhered twourkst ad nancacillicy stid gthay\n",
      "wecit d ticanaichove alicil Whinde, od tins thec r iorcthad, l Se pass.\n",
      "\n",
      "Gede\n",
      "boncy athes o tombjo urwislthioptherte bctoded on\n",
      "pe\n",
      "\n",
      "pr toavied n Brthe beg witha, opubys f\n",
      "\n",
      "ise as beche dof\n",
      "\n",
      "s, ontatofis latory bis\n",
      "ote fl in. m, on\n",
      "ben prar fece It ss wsufutr theshagrench d pin Corslththakexionorizerainse w to harctr Moterem sar inde rucomoflutifrcomede\n",
      "\n",
      "t\n",
      "us rere t as. tthe be Janorsiospa thansllingehit t\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79271139",
   "metadata": {},
   "source": [
    "# Note on attention\n",
    "\n",
    "For a token to pay 'attention' to other tokens, we need to relate them.\n",
    "\n",
    "It would be ilogical to relate a past token at the i-th index with a future token at the i-th + 1 index if we want to predict the i-th + 1 token. \n",
    "\n",
    "Instead, autoregressive models ough to do this using previous tokens, so the question becomes: \n",
    "\n",
    "how relate a set of tokens in a meaningful way to express what the next token ought to be?\n",
    "\n",
    "Self-attention is how to tie these to the last token in a sentence. \n",
    "\n",
    "\n",
    "We may for example average all previous token up to the last token of interest to capture their relative 'relationship' with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2e35a743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "xbow = torch.randn(B, T, C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0676f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbox = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]\n",
    "        xbow[b, t] = xprev.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b9e25ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cd4e0ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700515db",
   "metadata": {},
   "source": [
    "In the example above, the resulting tensor follows the sequence:\n",
    "\n",
    "a(1), a(1,2), a(1,2,3), a(1,2,3,4), ...\n",
    "where a = average\n",
    "\n",
    "So, each element is an average of all the elements before it (inclusive).\n",
    "\n",
    "This captures up to each element the context/relationship of the previous tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff19e1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "# @ is used for matrix multiplication. Equivalent to torch.matmul(a, b) and a.dot(b)\n",
    "c = a @ b\n",
    "\n",
    "# Referesher on matmuls:\n",
    "# the dot product is each element in the n-th row multiplied by the corresponding element in the m-th column\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0bde4c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "# @ is used for matrix multiplication. Equivalent to torch.matmul(a, b) and a.dot(b)\n",
    "c = a @ b\n",
    "\n",
    "# Referesher on matmuls:\n",
    "# the dot product is each element in the n-th row multiplied by the corresponding element in the m-th column\n",
    "\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ee8a3",
   "metadata": {},
   "source": [
    "Note on output:\n",
    "\n",
    "For the first row of c, the product is \n",
    "\n",
    "1 * 2 + 6 * 0 + 6 * 0 = 2 \n",
    "and \n",
    "1 * 7 + 4 * 0 + 5 * 0 = 6\n",
    "\n",
    "For the following rows of c:\n",
    "\n",
    "1 * 2 + 6 * 1 + 6 * 0 = 8\n",
    "and\n",
    "1 * 7 + 4 * 1 + 5 * 0 = 11\n",
    "\n",
    "Because we're multiplying downwards each row of a by each column of b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499cf5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "# This can be used to create a metrix that averages out another matrix.\n",
    "\n",
    "# This results in the average because it is the elements of a divided by the sum\n",
    "# Each row sums up to exactly one \n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83fc2c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x \n",
    "torch.allclose(xbow,xbow2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25717ef0",
   "metadata": {},
   "source": [
    "Note: this uses a triangular matrix wei where each row sums up to one to normalize the existing X tensor. (X = Train)\n",
    "\n",
    "Practically this means each block in X is averaged out by wei in the resulting matrix as a weighted sum. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dba49c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "print(xbow[0])\n",
    "print(xbow2[0])\n",
    "\n",
    "# The resulting matrices are the same because xbow was just an average of B, T, C - xbow 2 achieves the same thing using matrix multiplication by a matrix of averaged weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b57fb00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There is a third way to accomplish the above using softmax as well\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "# masked_fill = replace() -> tril == target, replacement\n",
    "# Here, all 0's are replaced with negative infinity\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# The softmax function normalizes all 1's to their probability of happening (basically replaces a value with its chance of happening) and all -inf to 0 \n",
    "# Same as wei = wei / wei.sum(1, keepdim=True)\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @  x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c98be4",
   "metadata": {},
   "source": [
    "Note: The reason why the triangular matrix is important here is because of the previous set up for off by one text prediction.\n",
    "\n",
    "That looks something like this: \n",
    "\n",
    "Input tensor([70]) | Target 80\n",
    "Input tensor([70, 80]) | Target 67\n",
    "Input tensor([70, 80, 67]) | Target 67\n",
    "Input tensor([70, 80, 67, 67]) | Target 2\n",
    "Input tensor([70, 80, 67, 67,  2]) | Target 69\n",
    "Input tensor([70, 80, 67, 67,  2, 69]) | Target 83\n",
    "Input tensor([70, 80, 67, 67,  2, 69, 83]) | Target 76\n",
    "Input tensor([70, 80, 67, 67,  2, 69, 83, 76]) | Target 64\n",
    "\n",
    "While a averaged tril matrix looks something like this:\n",
    "\n",
    "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
    "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
    "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n",
    "\n",
    "Notice the similarity? \n",
    "\n",
    "They both traverse horizontally, only 'averaging' for the elements at the i-th position. \n",
    "\n",
    "So, if we wanted to average out **only** some blocks (up to and including the t-th token), we could matmul it by it's corresponding row in the tril matrix.\n",
    "\n",
    "In short: tril is useful in this context because it can be used as a sort of sliding window to prevent future tokens from influecing the current prediction (this would lead to not learning but overfitting), as the zeros in the posterior positions to the t-th token ensure we only average out the relationships we care up to and before the token to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e547561d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-attention! \n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B, T,C)\n",
    "\n",
    "head_size = 16\n",
    "# nn.Linear creates a weight matrix of shape (out_features, in_features) with random initialization\n",
    "# When called, it computes: output = input @ weight.T\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)\n",
    "q = query(x)\n",
    "wei = q @ k.transpose(-2, -1)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "cb8d988e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9867ede8",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Based on the error (the mismatch between the off by one sequence (Y) and the original sequence (X) relative to what the model predicted with the resulted weight), the weights for the projections are then adjusted (back-propagation) so that whatever the weights for the query, key, value projections so they decrease the error when predicting again. The transposition is useful for matching the query with the key it's looking for, or rather, the result is the degree to which the query and key match since it's multiplying two vectors by each other.\n",
    "\n",
    "\n",
    "                    x <- Input matmul\n",
    "                    |\n",
    "                    |\n",
    "                 ---|------------------                    \n",
    "                /       \\              \\\n",
    "             query       key         value\n",
    "               |         |              |\n",
    "               -----------              |\n",
    "                    |                   |\n",
    "                    |                   |\n",
    "           r = query @ key(transposed)  |      <- Dot product between query, key\n",
    "                    |                   |\n",
    "                    |                   |\n",
    "                  r.trill               |\n",
    "                    |                   |\n",
    "                 softmax(r)             |\n",
    "                    |                   |\n",
    "                    |                   |\n",
    "                    ---------------------\n",
    "                              |\n",
    "                            r @ value\n",
    "\n",
    "\n",
    "\n",
    "# Todo\n",
    "- backpropagation = ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
